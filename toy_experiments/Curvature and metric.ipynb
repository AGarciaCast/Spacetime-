{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import importlib\n",
    "import math\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import root_scalar\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local modules\n",
    "import gm_utils\n",
    "importlib.reload(gm_utils)  # Forces Python to reload updated gm_utils.py\n",
    "from gm_utils import visualize_density, sample, geodesic, energy\n",
    "from curves import CubicSpline\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsfonts}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02692cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.array([0, 1, 2, 3])\n",
    "y_points = np.array([0, 2, 4, 6])\n",
    "f = interp1d(x_points, y_points)\n",
    "\n",
    "LAMBDA_MIN, LAMBDA_MAX = -30, 30 \n",
    "\n",
    "def log_SNR(t):\n",
    "    \"\"\"Implementation of the linear-logSNR noise schedule\"\"\"\n",
    "    return LAMBDA_MAX + (LAMBDA_MIN - LAMBDA_MAX) * t\n",
    "\n",
    "\n",
    "def alpha_sigma(t):\n",
    "    lambda_t = log_SNR(t)\n",
    "    alpha_t = torch.sigmoid(lambda_t).sqrt()\n",
    "    sigma_t = torch.sigmoid(-lambda_t).sqrt()\n",
    "    return alpha_t, sigma_t\n",
    "\n",
    "\n",
    "def gaussian_mixture_density(x, t):\n",
    "    \"\"\"Analytical implementation of the marginal log-density at time t\"\"\"\n",
    "    alpha_t, sigma_t = alpha_sigma(t)\n",
    "    means_t = alpha_t[:, None] * original_means[None, :]\n",
    "\n",
    "    variances_t = sigma_t[:, None]**2 + alpha_t[:, None]**2 * original_variance\n",
    "    log_probs = torch.log(weights[None, :]) - 0.5 * (torch.log(2 * torch.pi * variances_t) + (x[:, None] - means_t)**2 / variances_t)\n",
    "    log_p_t = torch.logsumexp(log_probs, dim=1)\n",
    "    return log_p_t\n",
    "\n",
    "\n",
    "def compute_vector_field(x, t):\n",
    "    \"\"\"Implementation of the PF-ODE vector field\"\"\"\n",
    "    alpha_t, sigma_t = alpha_sigma(t)\n",
    "    f_t = 0.5 * (LAMBDA_MIN - LAMBDA_MAX) * sigma_t**2\n",
    "    g2_t = (LAMBDA_MAX - LAMBDA_MIN) * sigma_t**2\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    log_p_t = gaussian_mixture_density(x, t)\n",
    "    grad_log_p_t = torch.autograd.grad(log_p_t.sum(), x, create_graph=True)[0]\n",
    "    \n",
    "    dx = f_t * x - 0.5 * g2_t * grad_log_p_t\n",
    "    dt = -torch.ones_like(dx)\n",
    "    return dt.detach().cpu().numpy(), dx.detach().cpu().numpy()\n",
    "\n",
    "def sample(x, n_steps, t_start=1, t_end=0):\n",
    "    \"\"\"PF-ODE sampling\"\"\"\n",
    "    t = t_start * torch.ones_like(x)\n",
    "    dt_val = (t_start - t_end) / n_steps\n",
    "    all_x = [x.detach().numpy()]\n",
    "    all_t = [t.detach().numpy()]\n",
    "    for i in range(n_steps):\n",
    "        dt, dx = compute_vector_field(x, t)\n",
    "        dt, dx = torch.from_numpy(dt), torch.from_numpy(dx)\n",
    "        x = x + dt * dx * dt_val\n",
    "        t = t + dt * dt_val\n",
    "        all_x.append(x.detach().numpy())\n",
    "        all_t.append(t.detach().numpy())\n",
    "    return np.array(all_t), np.array(all_x)\n",
    "\n",
    "def eds(t, x):\n",
    "    \"\"\"Implementation of the denoising mean, or Expected Denoised Sample (EDS) - based on Tweedie formula using the score function - Eq61 in the paper\"\"\"\n",
    "    assert t.shape == x.shape\n",
    "    assert t.ndim == 1\n",
    "    alpha_t, sigma_t = alpha_sigma(t)    \n",
    "    x.requires_grad_(True)\n",
    "    log_p_t = gaussian_mixture_density(x, t)\n",
    "    grad_log_p_t = torch.autograd.grad(log_p_t.sum(), x, create_graph=True)[0]\n",
    "    res = (x + sigma_t ** 2 * grad_log_p_t)\n",
    "    res = res / alpha_t\n",
    "    return res\n",
    "\n",
    "def mu(theta):\n",
    "    \"\"\"\n",
    "    Implementation of the expectation parameter - Eq 22 in the paper. Since our data distribution is 1D, the spacetime is 2D\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: torch.Tensor\n",
    "        a batch of spacetime points of shape (N, 2), where the first column is the `time` component, and the second is the `space` component\n",
    "    Returns\n",
    "    ----------\n",
    "    mu_t : torch.Tensor\n",
    "        `time` component of the expectation parameter - tensor or shape (N,)\n",
    "    mu_x : torch.Tensor\n",
    "        `space` component of the expectation parameter - tensor of shape (N,)\n",
    "    \"\"\"\n",
    "    t, x = theta[:, 0], theta[:, 1]\n",
    "    alpha_t, sigma_t = alpha_sigma(t)\n",
    "    x.requires_grad_(True)\n",
    "    f = eds(t, x)\n",
    "    div = torch.autograd.grad(f.sum(), x, create_graph=True)[0] # In 1D the divergence is just the derivative\n",
    "    mu_t, mu_x = sigma_t ** 2 / alpha_t * div + f ** 2, f\n",
    "    return mu_t, mu_x\n",
    "\n",
    "def eta(theta):\n",
    "    \"\"\"\n",
    "    Implementation of the natural parameter - Eq 18 in the paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: torch.Tensor\n",
    "        a batch of spacetime points of shape (N, 2), where the first column is the `time` component, and the second is the `space` component\n",
    "    Returns\n",
    "    ----------\n",
    "    eta_t : torch.Tensor\n",
    "        `time` component of the natural parameter - tensor or shape (N,)\n",
    "    eta_x : torch.Tensor\n",
    "        `space` component of the natural parameter - tensor of shape (N,)\n",
    "    \"\"\"\n",
    "    t, x = theta[:, 0], theta[:, 1]\n",
    "    alpha_t, sigma_t = alpha_sigma(t)\n",
    "    return -0.5 * alpha_t**2/sigma_t**2, alpha_t/sigma_t**2 * x\n",
    "\n",
    "\n",
    "\n",
    "def energy(theta):\n",
    "    \"\"\"\n",
    "    Implementation of the energy of a discretized curve - Eq 23 in the paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: torch.Tensor\n",
    "        a batch of spacetime points of shape (N, 2), where the first column is the `time` component, and the second is the `space` component\n",
    "    Returns\n",
    "    ----------\n",
    "    energy : torch.Tensor\n",
    "        Energy represented as tensor of shape (,)\n",
    "    \"\"\"\n",
    "    mu_t, mu_x = mu(theta)\n",
    "    eta_t, eta_x = eta(theta)\n",
    "    energies = (mu_t[1:] - mu_t[:-1]) * (eta_t[1:] - eta_t[:-1]) + (mu_x[1:] - mu_x[:-1]) * (eta_x[1:] - eta_x[:-1])\n",
    "    return energies.sum()\n",
    "\n",
    "def geodesic(theta1, theta2, n_opt_steps, num_intermediate_points, num_nodes=20):\n",
    "    \"\"\"\n",
    "    Implementation of approximate geodesic, parametrizes the curve as a CubicSpline and minimizes its energy w.r.t. curve's parameters using Adam optimizer\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta1: torch.Tensor\n",
    "        First endpoint of the curve, represented as a point in spacetime, i.e. (2,) tensor\n",
    "    theta2: torch.Tensor\n",
    "        Second endpoint of the curve, represented as a point in spacetime, i.e. (2,) tensor\n",
    "    n_opt_steps : int\n",
    "        Number of optimization steps\n",
    "    num_intermediate_points : int\n",
    "        Number of points to discretize the curve into (in the paper: `N`)\n",
    "    num_nodes : int\n",
    "        Parameter of the CubicSpline. The higher the number of nodes, the more flexible/expressive the curve\n",
    "    \"\"\"\n",
    "    curve = CubicSpline(begin=theta1, end=theta2, num_nodes=num_nodes)\n",
    "    optimizer = torch.optim.Adam(curve.parameters(), lr=1e-1)\n",
    "    t_tensor = torch.linspace(0, 1, num_intermediate_points).unsqueeze(0)\n",
    "    for step_id in tqdm.tqdm(range(n_opt_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        theta = curve(t_tensor)[0]\n",
    "        loss = energy(theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return curve(t_tensor)[0].detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def score_function(x, t):\n",
    "    \"\"\"Gradient of log p_t(x) w.r.t. x (the score).\"\"\"\n",
    "    x = torch.tensor([x], dtype=torch.float32, requires_grad=True)\n",
    "    t = torch.tensor([t], dtype=torch.float32)\n",
    "    log_p_t = gaussian_mixture_density(x, t)\n",
    "    grad_log_p_t = torch.autograd.grad(log_p_t.sum(), x)[0]\n",
    "    return grad_log_p_t.item()\n",
    "\n",
    "\n",
    "\n",
    "def plot_ode_geodesics(points):\n",
    "\n",
    "    for i, x in enumerate(points):\n",
    "        pf_ode_sample = sample(x, 512, t_start=1., t_end=t_min)\n",
    "        pf_ode_theta = torch.from_numpy(np.concatenate(pf_ode_sample, axis=1))\n",
    "        print('Energy of pf ode trajectory:', energy(pf_ode_theta).item(), end=' ')\n",
    "        theta1 = pf_ode_theta[0]\n",
    "        theta2 = pf_ode_theta[-1]\n",
    "        print('theta1', theta1)\n",
    "        print('theta2', theta2)\n",
    "        #theta1 = torch.tensor([0.00001, 2.2128]) #(theta=(t, x))\n",
    "        #theta2 = torch.tensor([0.9999, 3.5])\n",
    "        shortest_path12 = geodesic(theta1, theta2, n_opt_steps=1000, num_intermediate_points=128, num_nodes=2)\n",
    "        print('Energy of a geodesic:', energy(torch.from_numpy(shortest_path12)).item())\n",
    "        if i == 0:\n",
    "            label_pf = 'PF-ODE trajectory'\n",
    "            label_g = 'Spacetime Geodesic'\n",
    "        else:\n",
    "            label_pf = None\n",
    "            label_g = None\n",
    "        ax.plot(pf_ode_theta[:, 0], pf_ode_theta[:, 1], color='C3', label=label_pf)\n",
    "        ax.plot(shortest_path12[:, 0], shortest_path12[:, 1], color='C3', linestyle='--', label=label_g)\n",
    "\n",
    "\n",
    " \n",
    "def visualize_density(ax):\n",
    "    \"\"\"Estimates the marginal log-densities and visualizes it as background for the spacetime - used for both Fig1, and Fig5\"\"\"\n",
    "    n_time_points = 50\n",
    "    n_space_points = 200\n",
    "\n",
    "    t_vals = torch.linspace(0, 1, n_time_points)\n",
    "    x_vals = torch.linspace(-1.5, 1.5, n_space_points)\n",
    "    T, X = np.meshgrid(t_vals.numpy(), x_vals.numpy())\n",
    "    \n",
    "    T_flat = T.flatten()\n",
    "    X_flat = X.flatten()\n",
    "    densities = gaussian_mixture_density(torch.from_numpy(X_flat), torch.from_numpy(T_flat)).reshape(X.shape).detach()\n",
    "        \n",
    "    ax.contourf(T, X, (densities).reshape(X.shape), levels=25, cmap='viridis', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory_points(n_trajectories=100, n_steps=512, n_points_per_trajectory=10, \n",
    "                           x_range=(-2, 2), t_start=1.0, t_end=0, seed=None):\n",
    "    \"\"\"\n",
    "    Sample points along multiple PF-ODE trajectories.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_trajectories : int\n",
    "        Number of different trajectories to generate\n",
    "    n_steps : int  \n",
    "        Number of integration steps per trajectory\n",
    "    n_points_per_trajectory : int\n",
    "        Number of points to sample from each trajectory\n",
    "    x_range : tuple\n",
    "        Range of initial x values to sample from\n",
    "    t_start, t_end : float\n",
    "        Start and end times for the ODE integration\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sampled_points : torch.Tensor\n",
    "        Shape (n_trajectories * n_points_per_trajectory, 2) where each row is (t, x)\n",
    "    trajectory_ids : torch.Tensor\n",
    "        Shape (n_trajectories * n_points_per_trajectory,) indicating which trajectory each point came from\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    all_sampled_points = []\n",
    "    all_trajectory_ids = []\n",
    "    \n",
    "    for traj_id in range(n_trajectories):\n",
    "        x_init = torch.tensor([np.random.uniform(*x_range)], dtype=torch.float32)\n",
    "        t_trajectory, x_trajectory = sample(x_init, n_steps, t_start, t_end)\n",
    "        \n",
    "        # Combine into spacetime points (t, x)\n",
    "        trajectory_points = torch.from_numpy(np.column_stack([\n",
    "            t_trajectory.flatten(), \n",
    "            x_trajectory.flatten()\n",
    "        ]))\n",
    "        \n",
    "        # Sample n_points_per_trajectory random points from this trajectory\n",
    "        if len(trajectory_points) >= n_points_per_trajectory:\n",
    "            indices = torch.randperm(len(trajectory_points))[:n_points_per_trajectory]\n",
    "            sampled_points = trajectory_points[indices]\n",
    "        else:\n",
    "            sampled_points = trajectory_points\n",
    "            \n",
    "        all_sampled_points.append(sampled_points)\n",
    "        all_trajectory_ids.append(torch.full((len(sampled_points),), traj_id))\n",
    "    \n",
    "    sampled_points = torch.cat(all_sampled_points, dim=0)\n",
    "    trajectory_ids = torch.cat(all_trajectory_ids, dim=0)\n",
    "    \n",
    "    return sampled_points, trajectory_ids\n",
    "\n",
    "\n",
    "def visualize_sampled_points():\n",
    "    \"\"\"Example of how to use the sampling functions and visualize results\"\"\"\n",
    "    \n",
    "    # Sample points using both methods\n",
    "    points1, traj_ids1 = sample_trajectory_points(\n",
    "        n_trajectories=20,\n",
    "        n_points_per_trajectory=15,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Plot method 1: Random sampling from trajectories\n",
    "    visualize_density(ax1)\n",
    "    scatter1 = ax1.scatter(\n",
    "        points1[:, 0],\n",
    "        points1[:, 1],\n",
    "        c=traj_ids1,\n",
    "        s=20,\n",
    "        alpha=0.7,\n",
    "        cmap='tab10'\n",
    "    )\n",
    "    \n",
    "    ax1.set_title('Random Sampling from Trajectories')\n",
    "    ax1.set_xlabel('Time t')\n",
    "    ax1.set_ylabel('Space x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0fd64",
   "metadata": {},
   "source": [
    "## Metric and Sectional curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_tensor_differentiable(theta):\n",
    "    \"\"\"\n",
    "    theta : torch.Tensor\n",
    "        Shape (N, 2), each row = (t, x_t)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    I : torch.Tensor\n",
    "        Shape (N, 2, 2), metric tensor at each spacetime point\n",
    "    \"\"\"\n",
    "    theta = theta.clone().detach().requires_grad_(True)\n",
    "    N = theta.shape[0]\n",
    "\n",
    "    eta_t, eta_x = eta(theta)\n",
    "    mu_t, mu_x = mu(theta)\n",
    "\n",
    "    eta_vec = torch.stack([eta_t, eta_x], dim=1)  \n",
    "    mu_vec = torch.stack([mu_t, mu_x], dim=1)     #\n",
    "\n",
    "    I_batch = torch.zeros((N, 2, 2), dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "    for n in range(N):\n",
    "        # Compute Jacobians J_eta and J_mu\n",
    "        J_eta = torch.zeros((2, 2), dtype=theta.dtype, device=theta.device)\n",
    "        J_mu = torch.zeros((2, 2), dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        for k in range(2):\n",
    "            grad_eta_k = torch.autograd.grad(\n",
    "                eta_vec[n, k], theta, retain_graph=True, create_graph=True\n",
    "            )[0][n]  # gradient w.r.t. θ for η_k\n",
    "            grad_mu_k = torch.autograd.grad(\n",
    "                mu_vec[n, k], theta, retain_graph=True, create_graph=True\n",
    "            )[0][n]  # gradient w.r.t. θ for μ_k\n",
    "\n",
    "            J_eta[k, :] = grad_eta_k\n",
    "            J_mu[k, :] = grad_mu_k\n",
    "\n",
    "        #print('Jacobian eta:',J_eta,'Jacobian J_mu:', J_mu)\n",
    "        I_batch[n] = J_eta.T @ J_mu\n",
    "\n",
    "    return I_batch, theta \n",
    "\n",
    "def compute_sectional_curvature(theta):\n",
    "    \"\"\"\n",
    "    theta : torch.Tensor\n",
    "        Shape (N, 2), coordinates (t, x_t)\n",
    "    \"\"\"\n",
    "\n",
    "    g, theta_diff = metric_tensor_differentiable(theta)\n",
    "    N = g.shape[0]\n",
    "    g_inv = torch.inverse(g)  \n",
    "\n",
    "    christoffel = torch.zeros((N, 2, 2, 2), dtype=theta.dtype, device=theta.device)\n",
    "    g_derivs = torch.zeros((N, 2, 2, 2), dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "    #Compute all the derivatives of the metric\n",
    "    for n in range(N):\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                \n",
    "                grad_outputs = torch.zeros_like(g[n, i, j])\n",
    "                grad_outputs = grad_outputs + 1.0\n",
    "\n",
    "                grad_g_ij = torch.autograd.grad(\n",
    "                    g[n, i, j], theta_diff,\n",
    "                    grad_outputs=grad_outputs,\n",
    "                    retain_graph=True,\n",
    "                    create_graph=True,\n",
    "                    only_inputs=True\n",
    "                )[0][n]  \n",
    "\n",
    "                g_derivs[n, i, j, :] = grad_g_ij\n",
    "\n",
    "    # Compute Christoffel symbols: Γ^k_ij \n",
    "    for n in range(N):\n",
    "        for k in range(2):\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    sum_term = 0.0\n",
    "                    for l in range(2):\n",
    "                        bracket_term = (g_derivs[n, i, l, j] +    \n",
    "                                       g_derivs[n, j, l, i] -     \n",
    "                                       g_derivs[n, i, j, l])      \n",
    "                        sum_term += g_inv[n, k, l] * bracket_term\n",
    "\n",
    "                    christoffel[n, k, i, j] = 0.5 * sum_term\n",
    "\n",
    "    # Compute Riemann tensor component R^1_212\n",
    "\n",
    "    R_1212 = torch.zeros(N, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "    for n in range(N):\n",
    "        # i, j, k, l = 0, 1, 0, 1  \n",
    "        i, j, k, l = 0, 1, 0, 1\n",
    "\n",
    "        if christoffel[n, i, j, l].requires_grad:\n",
    "            dchrist_jl_k = torch.autograd.grad(\n",
    "                christoffel[n, i, j, l], theta_diff,\n",
    "                grad_outputs=torch.ones_like(christoffel[n, i, j, l]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0][n, k]\n",
    "        else:\n",
    "            dchrist_jl_k = torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        if christoffel[n, i, j, k].requires_grad:\n",
    "            dchrist_jk_l = torch.autograd.grad(\n",
    "                christoffel[n, i, j, k], theta_diff,\n",
    "                grad_outputs=torch.ones_like(christoffel[n, i, j, k]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0][n, l]\n",
    "        else:\n",
    "            dchrist_jk_l = torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        \n",
    "        quad1 = sum(christoffel[n, i, m, k] * christoffel[n, m, j, l] for m in range(2))\n",
    "        quad2 = sum(christoffel[n, i, m, l] * christoffel[n, m, j, k] for m in range(2))\n",
    "\n",
    "        R_1212[n] = dchrist_jl_k - dchrist_jk_l + quad1 - quad2\n",
    "\n",
    "   \n",
    "    # Compute the missing upper-index component R^1_101\n",
    "    R_2212 = torch.zeros(N, dtype=theta.dtype, device=theta.device) \n",
    "\n",
    "    for n in range(N):\n",
    "        i2, j2, k2, l2 = 1, 1, 0, 1 \n",
    "        if christoffel[n, i2, j2, l2].requires_grad:\n",
    "            dchrist_jl_k_2 = torch.autograd.grad(\n",
    "                christoffel[n, i2, j2, l2], theta_diff,\n",
    "                grad_outputs=torch.ones_like(christoffel[n, i2, j2, l2]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0][n, k2]\n",
    "        else:\n",
    "            dchrist_jl_k_2 = torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        if christoffel[n, i2, j2, k2].requires_grad:\n",
    "            dchrist_jk_l_2 = torch.autograd.grad(\n",
    "                christoffel[n, i2, j2, k2], theta_diff,\n",
    "                grad_outputs=torch.ones_like(christoffel[n, i2, j2, k2]),\n",
    "                retain_graph=True,\n",
    "                create_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0][n, l2]\n",
    "        else:\n",
    "            dchrist_jk_l_2 = torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        quad1_2 = sum(christoffel[n, i2, m, k2] * christoffel[n, m, j2, l2] for m in range(2))\n",
    "        quad2_2 = sum(christoffel[n, i2, m, l2] * christoffel[n, m, j2, k2] for m in range(2))\n",
    "\n",
    "        R_2212[n] = dchrist_jl_k_2 - dchrist_jk_l_2 + quad1_2 - quad2_2\n",
    "\n",
    "    # Lower the indices by multiplying with the metric:\n",
    "    #print('R1212', R_1212)\n",
    "    #print('R1012', R_up_i1)\n",
    "    R_lowered_1212 = g[:, 0, 0] * R_1212 + g[:, 0, 1] * R_2212\n",
    "    #print('first metric:', g[:, 0, 0])\n",
    "    #print(R_lowered_1212, torch.det(g)  )\n",
    "\n",
    "    # Sectional curvature K = R_{1212} / det(g) for 2D manifold (use lowered component)\n",
    "    \n",
    "    det_g = torch.det(g)  # (N,)\n",
    "    K = R_lowered_1212 / det_g\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "print('sectional 3', compute_sectional_curvature(torch.tensor([[0.2, -0.4]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106edbc",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_means = torch.tensor([0], dtype=torch.float32) #0,0,0   #-1,1,0\n",
    "original_variance = torch.tensor(0.5, dtype=torch.float32) #0.4\n",
    "weights = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "t_min = 0.1\n",
    "visualize_density(ax)\n",
    "\n",
    "plot_ode_geodesics(torch.tensor([[1.], [0.2], [-1]]))\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlim((t_min, 1))\n",
    "ax.set_xticks([t_min, 1], [r'$0$', r'$T$'], fontsize=13)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(r'$t$', fontsize=14, labelpad=-10)\n",
    "ax.set_ylabel(r'$\\mathbf{x}$', fontsize=14)\n",
    "ax.set_title('Sigmoid schedule lambda: -30,30', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('metric dif', metric_tensor_differentiable(torch.tensor([[0.5, 0.3]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3904851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trajectory sampling functions\n",
    "print(\"Testing trajectory point sampling...\")\n",
    "\n",
    "# Generate sampled points\n",
    "sampled_points, trajectory_ids = sample_trajectory_points(\n",
    "    n_trajectories=30, \n",
    "    n_points_per_trajectory=20, \n",
    "    x_range=(-1,1),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "visualize_sampled_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = torch.tensor([0.001, 1]) #(theta=(t, x))\n",
    "theta2 = torch.tensor([0.001, -1])\n",
    "\n",
    "num_discretization_points = 10000\n",
    "num_optimization_steps = 200\n",
    "\n",
    "shortest_path12 = geodesic(theta1, theta2, num_optimization_steps, num_discretization_points, num_nodes=200)\n",
    "\n",
    "\n",
    "def get_means_and_variances(t, x):\n",
    "    \"\"\"\n",
    "    When the data distribution p0 is a Gaussian mixture, then for every (t, xt), the `denoising distribution` (Eq 15 in the paper) is also a Gaussian Mixture. This function calculates the means and std of the denoising distribution in our Gaussian Mixture toy example. This function is used only to visualize the transition of densities in Figure 1.\n",
    "    \"\"\"\n",
    "    alpha, sigma = alpha_sigma(t)\n",
    "    snr = log_SNR(t).exp()\n",
    "    variance_inv = 1 / original_variance + snr\n",
    "    variance = 1 / variance_inv\n",
    "    means = original_means/original_variance + alpha/sigma ** 2 * x\n",
    "    return (means * variance).numpy(), variance.numpy()\n",
    "\n",
    "\n",
    "def visualize_density_w_geodesic(ax):\n",
    "    visualize_density(ax)\n",
    "    ax.set_xlabel(r\"$t$\", labelpad=-12, fontsize=14)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([0, 1], [r'$0$', r'$T$'], fontsize=14)\n",
    "    ax.scatter(*theta1.numpy(), color='blue', s=5)\n",
    "    ax.scatter(*theta2.numpy(), color='red', s=5)\n",
    "\n",
    "    points = shortest_path12\n",
    "\n",
    "    segments = np.concatenate([points[:-1, None], points[1:, None]], axis=1)\n",
    "    norm = plt.Normalize(0, len(points) - 1)\n",
    "    cmap = LinearSegmentedColormap.from_list('blue_red_gradient', ['blue', 'red'], N=len(points))\n",
    "\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm)\n",
    "    lc.set_array(np.arange(len(points)))\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    ax.add_collection(lc)\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.set_ylabel(r\"$\\mathbf{x}$\", fontsize=14)\n",
    "fig, (ax2, ax1) = plt.subplots(ncols=2, figsize=(5, 3), width_ratios=[1, 3])\n",
    "\n",
    "visualize_density_w_geodesic(ax1)\n",
    "\n",
    "num_plots = num_discretization_points\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('blue_red_gradient', ['blue', 'red'], N=num_plots)\n",
    "colors = [cmap(i / (num_plots - 1)) for i in range(num_plots)]\n",
    "\n",
    "indices = np.round(np.linspace(0, num_plots - 1, num_plots)).astype(np.int32)\n",
    "x = np.linspace(-6, 8, 1000)\n",
    "max_pdf = 0\n",
    "\n",
    "for idx in range(num_plots):\n",
    "    theta = torch.from_numpy(shortest_path12[idx])\n",
    "    mean, var = get_means_and_variances(theta[0], theta[1])\n",
    "    pdf = np.zeros_like(x)\n",
    "    for mean_i, weight_i in zip(mean, weights):\n",
    "        pdf += weight_i.item() * norm.pdf(x, loc=mean_i, scale=var ** 0.5)\n",
    "    pdf = pdf\n",
    "    max_pdf = max(max_pdf, np.max(pdf))\n",
    "    if idx == 0 or idx == num_plots - 1:\n",
    "        alpha = 1\n",
    "    else:\n",
    "        alpha = 0.3\n",
    "    ax2.plot(-pdf, x, color=colors[idx], alpha=alpha)\n",
    "    ax2.set_ylim((-4, 4))\n",
    "    ax2.set_xlim((-1.1 * max_pdf, 0))\n",
    "\n",
    "ax2.set_xlabel(r'$\\mathbf{x}_0 \\mid \\mathbf{x}_t$', fontsize=14)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "plt.subplots_adjust(wspace=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835769aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = np.linspace(0.1, 0.6,20)  \n",
    "xt_range = np.linspace(-0.6, 0.6, 20)    \n",
    "T, XT = np.meshgrid(t_range, xt_range)\n",
    "\n",
    "curvature_values = np.zeros_like(T)\n",
    "\n",
    "for i, t in enumerate(t_range):\n",
    "    for j, xt in enumerate(xt_range):\n",
    "        tensor_input = torch.tensor([[t, xt]], dtype=torch.float32)\n",
    "        curvature = compute_sectional_curvature(tensor_input)\n",
    "        if torch.is_tensor(curvature):\n",
    "            curvature_values[j, i] = curvature.item()\n",
    "        else:\n",
    "            curvature_values[j, i] = curvature\n",
    "        \n",
    "        if curvature_values[j, i] > 20:\n",
    "            curvature_values[j, i] = 20\n",
    "\n",
    "        if curvature_values[j, i] < -20:\n",
    "            curvature_values[j, i] = -20\n",
    "\n",
    "        print(f'sectional curvature at t={t:.2f}, xt={xt:.2f}: {curvature_values[j, i]:.4f}')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for i, x in enumerate(torch.tensor([[1.]])):\n",
    "    pf_ode_sample = sample(x, 512, t_start=0.5, t_end=t_min)\n",
    "    pf_ode_theta = torch.from_numpy(np.concatenate(pf_ode_sample, axis=1))\n",
    "    print('Energy of pf ode trajectory:', energy(pf_ode_theta).item(), end=' ')\n",
    "    theta1 = pf_ode_theta[0]\n",
    "    theta2 = pf_ode_theta[-1]\n",
    "    print('theta1', theta1)\n",
    "    print('theta2', theta2)\n",
    "    #theta1 = torch.tensor([0.00001, 2.2128]) #(theta=(t, x))\n",
    "    #theta2 = torch.tensor([0.9999, 3.5])\n",
    "    shortest_path12 = geodesic(theta1, theta2, n_opt_steps=1000, num_intermediate_points=128, num_nodes=2)\n",
    "    print('Energy of a geodesic:', energy(torch.from_numpy(shortest_path12)).item())\n",
    "    if i == 0:\n",
    "        label_pf = 'PF-ODE trajectory'\n",
    "        label_g = 'Spacetime Geodesic'\n",
    "    else:\n",
    "        label_pf = None\n",
    "        label_g = None\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(curvature_values, extent=[t_range.min(), t_range.max(), xt_range.min(), xt_range.max()],\n",
    "           origin='lower', aspect='auto', cmap='RdBu_r')\n",
    "\n",
    "plt.colorbar(label='Sectional Curvature')\n",
    "plt.title('Sectional Curvature Heatmap')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('xt')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "contour = plt.contourf(T, XT, curvature_values, levels=20, cmap='RdBu_r')\n",
    "plt.colorbar(contour, label='Sectional Curvature')\n",
    "plt.contour(T, XT, curvature_values, levels=20, colors='black', alpha=0.3, linewidths=0.5)\n",
    "plt.title('Sectional Curvature Contour Plot', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t', fontsize=12)\n",
    "plt.ylabel('xt', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHeatmap computed with:\")\n",
    "print(f\"- t range: {t_range.min():.1f} to {t_range.max():.1f} ({len(t_range)} points)\")\n",
    "print(f\"- xt range: {xt_range.min():.1f} to {xt_range.max():.1f} ({len(xt_range)} points)\")\n",
    "print(f\"- Curvature range: {curvature_values.min():.4f} to {curvature_values.max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
