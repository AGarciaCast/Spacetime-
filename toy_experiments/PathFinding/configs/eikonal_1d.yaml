
seed: 0
device: None                              # Set to 'cpu' or 'cuda' to override automatic device selection

logging:
  log_dir: './toy_experiments/logs'
  log_every_n_steps: 1
  visualize_every_n_steps: 1000
  no_progress_bar: False
  debug: False

geometry:
  dim_signal: 1
  x_min: [-1.0, 0.3]
  x_max: [1.0, 0.7]
  lambda_min: -10.0
  lambda_max: 10.0
  gmm_means: [0.0]
  gmm_variances: [0.5]
  gmm_weights: [1.0]

# Solver configuration
solver:
  backbone_type: 'pirate_net'                          # Options: 'mlp', 'pirate_net', 
  hidden_dim: 256
  num_layers: 4
  activation: 'ad-gauss-1'                     # Options: 'relu', 'sine', 'ad-gauss-1'
  nonlinearity: 0.0                            # Nonlinearity path in pirate net

  # Parameters for Fourier feature embedding
  use_fourier_features: True
  fourier_embed_scale: 1.0
  fourier_embed_dim: 256

  # parameters for reparameterization of weights with weight_fact
  use_reparam: True
  reparam_mean: 1.0
  reparam_std: 0.1
  
  # Normalization of input coordinates to [-1,1] for solver
  normalized: True
 
# Eikonal equiation configuration
eikonal:
  factored: True
  power: 2.0
  residual_loss: huber                      # Options: base, 'huber', 'clip'
  residual_huber_beta: 0.5                 
  residual_clip: 10.0
  vel_reg_weight: 0.0


# Data configuration
data:
  # Coordinate dataset config
  train_batch_size: 8192 # Number of coordinate pairs per batch
  test_batch_size: 8192
  num_pairs: 204800        # Number of coordinate pairs to sample for the dataset (in total)
  sampling_strategy: 'mixed'  # Options: 'uniform', 'trajectory', 'mixed'
  mixed_uniform_fraction: 0.5      # Only used when sampling_strategy='mixed'
  weighted_sampling_enabled: True
  weighted_sampling_every_n_epochs: 500
  weighted_sampling_warmup_epochs: 500
  weighted_sampling_min_weight: 0.2
  weighted_sampling_max_weight: 0.7
  weighted_sampling_batch_size: 4096
  trajectory_n_trajectories: 200
  trajectory_n_steps: 128
  trajectory_n_points_per_trajectory: 128
  trajectory_t_start: 1.0
  trajectory_t_end: 0.0
  trajectory_oversample_factor: 1.2
  # DataLoader configuration
  num_workers: max    
  pin_memory: True
  train_save_data: True
  train_force_recompute: False
  val_save_data: True
  val_force_recompute: False
  test_save_data: True
  test_force_recompute: False

# Training configuration
training:
  num_epochs: 3000
  model_checkpoint: True
  early_stopping: False
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0              # Increased from 0.2 - clips gradient NORM not values
  gradient_clip_algorithm: 'norm'      # Explicitly use norm-based clipping


# Testing configuration
test:
  val_every_n_epochs: 500


# Optimization configuration
optimizer:
  name: soap                                     # Options: 'adam', 'adamw', 'muon'
  learning_rate: 1e-4                           # Reduced from 3e-4 for more stable convergence
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0                           
  # Learning rate scheduler configuration
  use_lr_scheduler: True
  scheduler_type: cosine                    # Options: 'exponential', 'cosine'
  warmup_pct: 5
  min_lr: 3e-5
  min_lr_at_pct: 90
