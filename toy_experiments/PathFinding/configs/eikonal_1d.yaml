
seed: 0
device: None                              # Set to 'cpu' or 'cuda' to override automatic device selection

logging:
  log_dir: './toy_experiments/logs'
  log_every_n_steps: 1
  visualize_every_n_steps: 1000
  no_progress_bar: False
  debug: False

geometry:
  dim_signal: 1
  x_min: [-1.0, 0.3]
  x_max: [1.0, 0.7]
  lambda_min: -10.0
  lambda_max: 10.0
  gmm_means: [0.0]
  gmm_variances: [0.5]
  gmm_weights: [1.0]

# Solver configuration
solver:
  backbone_type: 'pirate_net'                          # Options: 'mlp', 'pirate_net', 
  hidden_dim: 256
  num_layers: 4
  activation: 'ad-gauss-1'                     # Options: 'relu', 'sine', 'ad-gauss-1'
  nonlinearity: 0.0                            # Nonlinearity path in pirate net

  # Parameters for Fourier feature embedding
  use_fourier_features: True
  fourier_embed_scale: 1.0
  fourier_embed_dim: 256

  # parameters for reparameterization of weights with weight_fact
  use_reparam: True
  reparam_mean: 1.0
  reparam_std: 0.1
  
  # Normalization of input coordinates to [-1,1] for solver
  normalized: True
 
# Eikonal equiation configuration
eikonal:
  factored: True
  power: 2
  residual_loss: huber
  residual_huber_beta: 1.0
  residual_clip: 10.0
  vel_reg_weight: 0.0


# Data configuration
data:
  # Coordinate dataset config
  train_batch_size: 2560 # Number of coordinate pairs per batch
  test_batch_size: 2560
  num_pairs: 102400        # Number of coordinate pairs to sample for the dataset (in total)
  sampling_strategy: 'uniform'  # Options: 'uniform', 'curvature'
  # DataLoader configuration
  num_workers: 8    
  pin_memory: True
  train_save_data: True
  train_force_recompute: True
  val_save_data: True
  val_force_recompute: True
  test_save_data: True
  test_force_recompute: True

# Training configuration
training:
  num_epochs: 100
  model_checkpoint: True
  early_stopping: True
  accumulate_grad_batches: 1
  gradient_clip_val: 0.5


# Testing configuration
test:
  val_every_n_epochs: 100


# Optimization configuration
optimizer:
  name: soap                                     # Options: 'adam', 'soap', 'muon'
  learning_rate: 1e-3
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.0
  # Learning rate scheduler configuration
  use_lr_scheduler: True
  scheduler_type: exponential                    # Options: 'exponential', 'cosine'
  warmup_steps: 500
  decay_rate: 0.1
  decay_steps: 5000
  min_lr: 1e-6
